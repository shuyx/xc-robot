# **XC-ROBOT 智能交互模块设计方案**

**版本**: 1.0  
**日期**: 2025-07-18  
**来源**: 本文档基于用户对AI交互的需求，并结合`XC_OS_SYSTEM_DESIGN.md`中的AI集成架构进行规划。

---

## 1. 功能概述

“智能交互”模块是XC-OS人机交互的核心，旨在通过集成先进的AI能力（人脸识别、自然语言处理、语音识别），将机器人从一个被动执行指令的工具，转变为一个能够主动识别用户、理解意图并自主完成任务的智能伙伴。

该模块将取代旧的“语音控制”和“手势识别”菜单，整合为一个功能更强大、更统一的交互中心。下设三个核心二级子菜单：

1.  **人脸识别与权限 (Face Recognition & Permissions)**: 负责识别用户身份并管理其权限和历史记录。
2.  **对话式任务 (Conversational Tasking)**: 系统的核心交互入口，通过聊天界面接收文本和语音指令，理解并执行复杂任务。
3.  **梯控系统 (Elevator Control)**: （保留）用于与楼宇电梯系统联动。

---

## 2. 功能界面布局与细节规划

### 2.1 子菜单一：人脸识别与权限

**目标**: 快速识别操作员，个性化加载其工作信息和权限，并提供历史任务追溯。

**页面布局**:
- **左侧面板 (相机与控制区)**:
    - **相机实时画面**: 一个窗口实时显示机器人头部相机（如Gemini 335的RGB流）的画面。
    - **控制按钮**: 一个醒目的 **[扫描并识别用户]** 按钮。
    - **状态提示**: 一行文本，显示状态如 `待命中`, `正在扫描...`, `识别成功`, `未识别用户`。

- **右侧主面板 (用户信息区)**:
    - **默认状态**: 显示“请点击左侧按钮开始扫描”。
    - **识别成功后**: 面板会动态加载并显示以下信息卡片：
        - **用户信息卡片**:
            - `姓名: [Kevin Yuan]`
            - `授权工位: [A-12]`
            - `最后记录位置: [实验室]`
            - `调度权限: [管理员 | 普通用户]`
        - **历史任务卡片**:
            - 标题: `最近5次呼叫任务`
            - 一个可滚动的列表，每项包含：
                - `时间: 2025-07-18 10:30`
                - `任务: 从“物料架B”抓取“扳手”到“工位A-12”`
                - `状态: [已完成 | 已取消]`

### 2.2 子菜单二：对话式任务

**目标**: 提供一个监控和配置中心，用于管理通过外部聊天界面下发的任务。

**页面布局**:
- **顶部面板 (配置与连接区)**:
    - **移动端接入**: 
        - `聊天页面地址: [https://<your-robot-ip>:8888/chat]` (只读)
        - **[生成二维码]** 按钮：点击后弹出一个二维码，方便手机扫描快速访问聊天页面。
    - **系统状态**: `当前状态: [监听中 | 正在解析指令 | 等待用户确认 | 任务执行中]`

- **主面板 (任务监控区)**:
    - **左侧：实时对话记录**: 一个只读的聊天记录窗口，实时同步手机端聊天页面的对话内容。
        ```
        [你]: 帮我从实验室拿一个万用表过来
        [机器人]: 好的，我将执行以下任务：
                 - 动作: 抓取并递送
                 - 目标: 万用表
                 - 起点: 实验室
                 - 终点: 您的当前位置
                 请回复“确认”以开始。
        [你]: 确认
        ```
    - **右侧：解析任务详情**: 一个结构化的信息面板，显示LLM解析出的当前任务。
        - `任务ID: [T-20250718-005]`
        - `动作 (Action): [Grasp & Deliver]`
        - `目标 (Object): [万用表]`
        - `起点 (Source): [实验室]`
        - `终点 (Destination): [用户当前位置]`
        - `状态: [已确认，正在执行]`
        - **[强制终止任务]** 按钮：一个红色的紧急停止按钮。

### 2.3 子菜单三：梯控系统

**目标**: （保留并标准化）提供与建筑内电梯控制系统的接口。

**页面布局**:
- **电梯状态面板**:
    - `当前楼层: [5]`
    - `运行状态: [静止 | 上行 | 下行]`
    - `门状态: [已关闭 | 已开启]`
- **控制面板**:
    - **呼叫电梯**: `[呼叫上行]`, `[呼叫下行]`
    - **楼层选择**: 一组数字按钮 `[1] [2] [3] [4] [5] ...`
    - `[开门]` `[关门]` 按钮

---

## 3. 技术路线与实现方案

### 3.1 总体架构

我们将采用一个**微服务**的思路。除了现有的PyQt主应用，我们会启动一个独立的、轻量级的**Python Web服务器 (使用FastAPI或Flask)**。这个服务器专门负责处理来自移动端聊天页面的HTTP和WebSocket请求。

```
+----------------------+      (HTTP/WebSocket)      +-------------------------+
| 移动端Web聊天页面    | <----------------------> |  Python Web服务器       |
| (HTML/JS)            |                            |  (FastAPI/Flask)        |
+----------------------+                            |   - 语音转文本 (Whisper)  |
                                                    |   - LLM任务解析         |
                                                    +------------+------------+
                                                                 |
                                                                 | (内部进程通信)
                                                                 V
+-----------------------------------------------------------------+
|                        XC-ROBOT 主应用 (PyQt)                     |
|  +-------------------------+      +---------------------------+ |
|  |   WebBridge (扩展)      |----->|   智能交互控制模块        | |
|  +-------------------------+      |   - 人脸识别模块          | |
|                                   |   - 对话任务状态机        | |
|                                   |   - 机器人调度接口        | |
|                                   +-------------+-------------+ |
|                                                 |               |
|                                                 V               |
|                                   +---------------------------+ |
|                                   |  集成控制器 (现有)        | |
|                                   +---------------------------+ |
+-----------------------------------------------------------------+
```

### 3.2 人脸识别与权限模块实现

1.  **后端 (`WebBridge` 和新模块)**:
    - **`start_face_scan()`**: 在`WebBridge`中新增此`@pyqtSlot`。当被调用时，它会触发一个新的`FaceRecognitionController`。
    - **`FaceRecognitionController`**: 
        - 调用`gemini335_control`模块获取实时RGB视频帧。
        - 使用`face_recognition`库（基于dlib）进行人脸检测和编码。
        - 将检测到的面部编码与预先存储的用户面部编码数据库（可以是一个JSON文件）进行比对。
        - 识别成功后，从用户数据库中查询用户信息（工位、权限、历史任务），并通过Qt信号发送给前端。
2.  **前端 (JS)**:
    - “扫描”按钮点击后调用 `bridge.start_face_scan()`。
    - 监听后端发送的用户信息信号，接收到数据后动态填充到右侧的用户信息面板中。

### 3.3 对话式任务模块实现

1.  **移动端聊天页面 (HTML/JS)**:
    - 一个简单的HTML页面，包含聊天记录显示窗口和输入框。
    - 使用JavaScript通过**WebSocket**连接到后端的Python Web服务器，实现实时双向通信。
    - 支持文本输入和**语音录制**。录制的语音数据（如.wav或.mp3文件）通过HTTP POST请求上传到Web服务器。

2.  **Python Web服务器 (FastAPI)**:
    - **WebSocket端点**: 接收来自用户的文本消息，并向用户推送机器人的回复。
    - **HTTP上传端点**: 接收语音文件。
    - **语音转文本 (STT)**: 收到语音文件后，使用`openai-whisper`库或类似工具将其转换为文本。
    - **LLM任务解析**: 
        - 将STT转换后的文本或用户直接输入的文本，发送给一个大语言模型（如通过API调用GPT-4，或使用本地模型）。
        - **关键在于Prompt工程**: 设计一个精良的系统提示(System Prompt)，要求LLM从对话中提取结构化的任务信息，并以JSON格式返回。例如：
          > "你是一个机器人任务解析助手。请从用户的请求中提取动作、目标、起点和终点，并以JSON格式返回。如果信息不全，请生成一个澄清问题。" 
        - LLM返回的JSON可能像这样: `{"action": "fetch", "object": "multimeter", "source": "lab", "destination": "user"}`。
    - **任务确认流程**: 
        - 收到LLM解析的JSON后，生成一条人类可读的确认消息（“好的，我将从实验室拿一个万用表给您...”），通过WebSocket发回给用户。
        - 等待用户回复“确认”。
    - **任务分发**: 收到用户的“确认”后，通过内部进程通信（如ZeroMQ或简单的本地Socket）将结构化的任务JSON发送给主应用的“智能交互控制模块”。

3.  **主应用 - 智能交互控制模块 (Python/PyQt)**:
    - **任务接收器**: 监听来自Web服务器的任务指令。
    - **任务状态机**: 管理任务的生命周期（接收 -> 执行 -> 完成/失败）。
    - **机器人调度器**: 这是最关键的一步。它将抽象的任务JSON**翻译**成对`IntegratedController`的具体调用序列。例如，对于`{"action": "fetch", ...}`任务，它会：
        1.  查询“实验室”和“用户位置”在地图上的具体坐标。
        2.  调用`integrated_controller.chassis.move_to(lab_coords)`。
        3.  等待移动完成。
        4.  调用`integrated_controller.dual_arm.perform_grasp("multimeter")`。
        5.  等待抓取完成。
        6.  调用`integrated_controller.chassis.move_to(user_coords)`。
        7.  ...等等。

### 3.4 前端UI实现

- **技术**: 完全在现有的`UI/xc_os_newui.html`中通过HTML/CSS/JS实现。
- **动态内容**: 大量使用JS来动态创建和更新DOM元素，以响应来自`WebBridge`的信号和数据。
- **状态同步**: 通过定时轮询或由后端主动推送的信号，保持UI与机器人后台状态的一致性。
